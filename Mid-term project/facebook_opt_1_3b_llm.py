# -*- coding: utf-8 -*-
"""facebook/opt-1.3b_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PSiPvo2YD-QBFXZaf_fmh-zwIU0vFXjW
"""

#LLM: facebook/opt-1.3b
!pip install transformers torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Loading the OPT model and tokenizer with left padding
model_name = "facebook/opt-1.3b"
tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')
model = AutoModelForCausalLM.from_pretrained(model_name)
if torch.cuda.is_available(): # for running with GPU if available
    model = model.to('cuda')

def read_input_file(file_path):
    with open(file_path, 'r') as f:
        sentences = [line.strip() for line in f.readlines() if line.strip()]
    return sentences
def tokenize_sentences(sentences): #Tokenize sentences and create attention masks
    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")

    if torch.cuda.is_available():
        inputs = {key: val.to('cuda') for key, val in inputs.items()}
    return inputs

# for generating complete sentences
def generate_completions(inputs, max_length=50, batch_size=32):
    model.eval()

    # Generating outputs in batches
    completions = []
    with torch.no_grad():
        for i in range(0, inputs['input_ids'].shape[0], batch_size):
            # Get the current batch
            batch_input_ids = inputs['input_ids'][i:i + batch_size]
            batch_attention_mask = inputs['attention_mask'][i:i + batch_size]

            # Generate the outputs
            output = model.generate(
                input_ids=batch_input_ids,
                attention_mask=batch_attention_mask,
                max_length=max_length + len(batch_input_ids[0]),  # Adjust max length to include input
                num_return_sequences=1,
                do_sample=True,
                top_k=50,
                top_p=0.95
            )

            # Decode the generated outputs
            for out in output:
                generated_sentence = tokenizer.decode(out, skip_special_tokens=True)
                completions.append(generated_sentence)

    return completions

#Main function to run the pipeline
def main():
    input_file = '/content/truncated_sentences.txt'  # Input file with half-English sentences
    output_file = '/content/facebookopt_sentences.txt'  # Output file for generated sentences

    # Load the sentences from the input file
    sentences = read_input_file(input_file)

    # Tokenize the input sentences
    inputs = tokenize_sentences(sentences)

    # Generate completions for the input sentences
    generated_sentences = generate_completions(inputs)

    #Writing the generated sentences to an output file
    with open(output_file, 'w') as f:
        for original, generated in zip(sentences, generated_sentences):
            f.write(f"Original: {original}\n")
            f.write(f"Completed: {generated}\n\n")

    print(f"Completed sentences have been written to {output_file}.")

#running the main function
if __name__ == "__main__":
    main()

"""After getting this facebookopt_sentences.txt, extract the completed sentences which contains both (xi,xj) pair in each sentence.From this, get only the sentences in matched_facebookopt_sentences.txt.

In next step,xj's are extracted from these sentences and saved to facebookopt_xj.txt.
"""