# -*- coding: utf-8 -*-
"""gpt2_gptneo_t5_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18VoUwuvyx1W3JcfFmjRsT9v2ZwwYih6n
"""

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the pre-trained DistilGPT-2 model and tokenizer
model_name = "distilgpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Ensure the model is on the GPU (MX 450)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Function to read input sentences from a text file
def read_sentences(input_file):
    with open(input_file, 'r', encoding='utf-8') as file:
        sentences = file.readlines()
    # Filter out empty lines and strip extra whitespace
    return [sentence.strip() for sentence in sentences if sentence.strip()]

# Function to write output sentences to a text file
def write_sentences(output_file, output_sentences):
    with open(output_file, 'a', encoding='utf-8') as file:
        for sentence in output_sentences:
            file.write(sentence + '\n')

# Function to generate completions for a batch of sentences
def generate_completions(batch_sentences, model, tokenizer, max_length=25):
    completed_sentences = []
    for sentence in batch_sentences:
        input_ids = tokenizer.encode(sentence, return_tensors='pt').to(device)

        # Skip if the sentence could not be tokenized properly
        if input_ids.shape[-1] == 0:
            print(f"Skipping empty input for sentence: {sentence}")
            continue

        # Prepare attention mask
        attention_mask = torch.ones(input_ids.shape, device=device)

        # Generate output with attention_mask and pad_token_id set
        output_ids = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=max_length,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id  # Explicitly set pad_token_id
        )
        completed_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        completed_sentences.append(completed_sentence)
    return completed_sentences

# Batch process sentences
def batch_process(input_file, output_file, batch_size=32):
    sentences = read_sentences(input_file)
    total_sentences = len(sentences)
    for i in range(0, total_sentences, batch_size):
        batch = sentences[i:i + batch_size]
        print(f"Processing batch {i // batch_size + 1}/{(total_sentences // batch_size) + 1}")
        completed_batch = generate_completions(batch, model, tokenizer)
        write_sentences(output_file, completed_batch)

# File paths
input_file_path = '/truncated_sentences.txt'  # Replace with your file containing 10,000 sentences
output_file_path = '/completed_sentences.txt'  # Output file where completed sentences will be saved

# Run the batch processing
batch_process(input_file_path, output_file_path, batch_size=32)

print(f"Completed processing 10,000 sentences. Results saved to {output_file_path}.")

import torch
from transformers import GPTNeoForCausalLM, GPT2Tokenizer, T5Tokenizer, T5ForConditionalGeneration

# Load both GPT-Neo and T5 models and tokenizers
def load_models():
    # GPT-Neo model and tokenizer
    gptneo_model_name = "EleutherAI/gpt-neo-125M"
    gptneo_tokenizer = GPT2Tokenizer.from_pretrained(gptneo_model_name, padding_side='left')  # Set padding to left
    gptneo_model = GPTNeoForCausalLM.from_pretrained(gptneo_model_name)

    # Set pad_token for GPT-Neo tokenizer
    gptneo_tokenizer.pad_token = gptneo_tokenizer.eos_token  # Set padding token to eos token

    # T5-small model and tokenizer
    t5_model_name = "t5-small"
    t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)
    t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)

    # Set pad_token for T5 tokenizer
    t5_tokenizer.pad_token = t5_tokenizer.eos_token  # Use eos token as pad token

    device = "cuda" if torch.cuda.is_available() else "cpu"
    gptneo_model.to(device)
    t5_model.to(device)

    return gptneo_model, gptneo_tokenizer, t5_model, t5_tokenizer, device

# Load sentences from the input text file
def load_sentences(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        sentences = file.readlines()
    return [sentence.strip() for sentence in sentences if sentence.strip()]

# Save results to an output file
def save_results(output_file, results):
    with open(output_file, 'w', encoding='utf-8') as file:
        for line in results:
            file.write(line + '\n')

# Generate completions using GPT-Neo in batches
def generate_with_gptneo(gptneo_model, gptneo_tokenizer, device, sentences):
    outputs = []
    for i in range(0, len(sentences), 32):  # Process in batches of 32
        batch = sentences[i:i + 32]
        input_ids = gptneo_tokenizer(batch, padding=True, truncation=True, return_tensors='pt')['input_ids'].to(device)
        attention_mask = torch.ones(input_ids.shape, device=device)

        output_ids = gptneo_model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=50,
            num_return_sequences=1,
            pad_token_id=gptneo_tokenizer.eos_token_id
        )

        outputs.extend([gptneo_tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output_ids])
    return outputs

# Generate completions using T5-small in batches
def generate_with_t5(t5_model, t5_tokenizer, device, sentences):
    outputs = []
    for i in range(0, len(sentences), 32):  # Process in batches of 32
        batch = sentences[i:i + 32]
        input_ids = t5_tokenizer(["complete: " + sentence for sentence in batch], padding=True, truncation=True, return_tensors='pt')['input_ids'].to(device)
        attention_mask = torch.ones(input_ids.shape, device=device)

        output_ids = t5_model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=50,
            num_return_sequences=1,
            pad_token_id=t5_tokenizer.eos_token_id
        )

        outputs.extend([t5_tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output_ids])
    return outputs

# Main function to process the input file and run both models
def process_input_file(input_file, gptneo_output_file, t5_output_file):
    # Load models and tokenizers
    gptneo_model, gptneo_tokenizer, t5_model, t5_tokenizer, device = load_models()

    # Load input sentences
    sentences = load_sentences(input_file)

    # Generate completions in batches
    gptneo_results = generate_with_gptneo(gptneo_model, gptneo_tokenizer, device, sentences)
    t5_results = generate_with_t5(t5_model, t5_tokenizer, device, sentences)

    # Save results to the respective output files
    gptneo_output = [f"Input: {sentences[i]}\nGPT-Neo Output: {gptneo_results[i]}\n" for i in range(len(sentences))]
    t5_output = [f"Input: {sentences[i]}\nT5 Output: {t5_results[i]}\n" for i in range(len(sentences))]

    save_results(gptneo_output_file, gptneo_output)
    save_results(t5_output_file, t5_output)

# Define input and output file paths
input_file_path = '/truncated_sentences.txt'  # Replace with your input file
gptneo_output_file_path = '/gptneo_output.txt'  # GPT-Neo output file
t5_output_file_path = '/t5_output.txt'  # T5-small output file

# Run the process
process_input_file(input_file_path, gptneo_output_file_path, t5_output_file_path)

print(f"Completed processing. GPT-Neo results saved to {gptneo_output_file_path}, T5 results saved to {t5_output_file_path}")

# Let's process the file to only extract lines with "GPT-Neo Output" content
input_file_path = "/gptneo_output.txt"
output_file_path = "/gptneo_only_output.txt"

# Open the input file to read and output file to write filtered lines
with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:
    for line in infile:
        if line.startswith("GPT-Neo Output:"):
            # Write only the output part after the "GPT-Neo Output:" prefix
            outfile.write(line.replace("GPT-Neo Output:", "").strip() + "\n")

output_file_path

# Path to your text file
file_path = '/gptneo_only_output.txt'

# Read the file and calculate the length
with open(file_path, 'r', encoding='utf-8') as file:
    content = file.readlines()

# Calculate the number of characters in the file
file_length = len(content)
print(f"File length (number of characters): {file_length}")